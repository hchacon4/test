{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "decoder11(1k_samps_area0.1_0.01)random_sparse_grid_comparison_20_pipes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4Epm5SJkrzLx",
        "VQzqACaz6D71",
        "BTY1vNInM3RI",
        "ydc9rlm86Hmt",
        "3ocO0Om3ofeX",
        "RVZsPo1p2RoP",
        "5wCj-uKc561c",
        "7KkFT5x93F5H",
        "if63qNQuS0HA",
        "j_Zwmb6sYpbm",
        "qysp3xV5W2dg",
        "k5xwqcQyNoBV"
      ],
      "mount_file_id": "1gaszVphVA2D62-Ev_Wiqzwh9SgeJTd8X",
      "authorship_tag": "ABX9TyPU3eUbrU+7KJrkJ9//JzVN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hchacon4/test/blob/main/decoder11(1k_samps_area0_1_0_01)random_sparse_grid_comparison_20_pipes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Epm5SJkrzLx"
      },
      "source": [
        "####Notes\n",
        "- Using SimData_to_csv notebook to create dataset csv\n",
        "- I may want a labels map (dict) for pipe labels like the one here\n",
        "  - https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#iterating-and-visualizing-the-dataset\n",
        "- Unclear what a one-hot encoded tensor might be used for\n",
        "  - used here as a target label transformation\n",
        "    - https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html#lambda-transforms\n",
        "\n",
        "- How do I handle two labels?\n",
        "- Is normalization required?\n",
        " - ans: desirable when features have different ranges. https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029#:~:text=The%20goal%20of%20normalization%20is,when%20features%20have%20different%20ranges.\n",
        "- Complete a panda tutorial\n",
        "- He: Classifer tutorial; CIFAR10 dataset (3 channel images)\n",
        " - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "- Colab Pro\n",
        " - One important caveat to remember while using Colab is that the files you upload to it wonâ€™t be available forever. Colab is a temporary environment with an idle timeout of 90 minutes and an absolute timeout of 12 hours. This means that the runtime will disconnect if it has remained idle for 90 minutes, or if it has been in use for 12 hours [longer for Colab Pro]. On disconnection, you lose all your variables, states, installed packages, and files and will be connected to an entirely new and clean environment on reconnecting. source:https://neptune.ai/blog/google-colab-dealing-with-files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQzqACaz6D71"
      },
      "source": [
        "####Model framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPV_XlfZFlI-",
        "outputId": "af456b28-23ea-42c5-9613-1e533080ddaa"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import pandas as pd   # For loading csv file dataset\n",
        "import random\n",
        "\n",
        "# Define model\n",
        "#  -- when called, model returns a output_dim dimensional tensor\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_lrelu_stack = nn.Sequential(\n",
        "            # Define the input dimensions\n",
        "            # nn.Linear(input_dim, 256),\n",
        "            nn.Linear(input_dim, 512),\n",
        "            # nn.Linear(input_dim, output_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            # nn.Linear(512, 512),\n",
        "            # nn.LeakyReLU(),\n",
        "            # nn.Linear(512, 512),\n",
        "            # nn.LeakyReLU(),\n",
        "            # nn.Linear(512, 512),\n",
        "            # nn.LeakyReLU(),\n",
        "            # nn.Linear(512, 512),\n",
        "            # nn.LeakyReLU(),\n",
        "            # nn.Linear(512, 512),\n",
        "            # nn.LeakyReLU(),\n",
        "            # # Define the output dimensions\n",
        "            # nn.Linear(256, output_dim),\n",
        "            nn.Linear(512, output_dim),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        # Yisong: initialize the weights in the first layer, and the following layers will follow suit.\n",
        "        # self.linear_lrelu_stack[0].weight.data /= 100.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        # print(x.size())\n",
        "        logits = self.linear_lrelu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTY1vNInM3RI"
      },
      "source": [
        "####Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi2L7VdNGqNb"
      },
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, epoch=0, mod=100):\n",
        "    \"\"\"\n",
        "    loss_fn is a list of pytorch loss functions.\n",
        "    \"\"\"\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)   # For avg training loss\n",
        "    # print(f'train_loop(): dataset size: {size}')\n",
        "    # print(f'train_loop(): num_batches: {num_batches}')\n",
        "    train_loss, train_accuracy = 0, 0\n",
        "    confusion_matrix = torch.zeros(20, 20, dtype=torch.int32)\n",
        "    prevLoss = 100.\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # print(batch, X)\n",
        "        # print(y)\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        # print(y.size())\n",
        "        # print(pred.size())\n",
        "        # print(pred)\n",
        "        loss = loss_fn(pred, y)   # returns single value; avg loss across batch      \n",
        "        # print(loss * len(y))\n",
        "        # for i in range(len(y)) :\n",
        "        #   print(pred[i])\n",
        "        #   print(y[i])\n",
        "        #   ls = loss_fn(pred[i], y[i])\n",
        "        #   print(f'loss {i} {ls}')\n",
        "        # print(y)\n",
        "        # print(pred.argmax(1).type(y.dtype))\n",
        "        # # Confusion Matrix\n",
        "        # for i in range(len(y)) :\n",
        "        #   preds = pred.argmax(1).type(y.dtype)\n",
        "        #   confusion_matrix[y[i]][preds[i]] += 1\n",
        "        # print(confusion_matrix)\n",
        "        \n",
        "        # Top-k predictions\n",
        "        # torch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n",
        "        k = 1\n",
        "        # top_k = torch.topk(input=pred, k=k, dim=1,)\n",
        "        # print(top_k)\n",
        "\n",
        "        # Disaggregate performance -- save model\n",
        "        #  goal: extract outliers (in another notebook)\n",
        "        #  Make into a function.\n",
        "        # if epoch > 2000 and loss.item() > (prevLoss * 15) :\n",
        "        # if epoch == 10000 :\n",
        "        #   print(f'train_loop(): epoch {epoch} -- loss jumped from {prevLoss:.3} to {loss.item():.3}')\n",
        "        #   torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{epoch}_{batch}_{loss.item():.3}.pt')\n",
        "        # # print(prevLoss)\n",
        "        # prevLoss = loss.item()\n",
        "        # print(prevLoss > loss * 20)\n",
        "        # print(epoch)\n",
        "        # assert False\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # train_loss += loss_fn(pred, y).item() * len(y)   # I think this second call to loss_fn runs the grad twice. ???\n",
        "        train_loss += loss.item() * len(y)\n",
        "        # y.dtype ensures the the operand types match for use w/ comparison operator (sensitive to type)\n",
        "        train_accuracy += (pred.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n",
        "        # print(loss)\n",
        "\n",
        "        if batch % mod == 0:\n",
        "            # print(batch)\n",
        "            # print(y)\n",
        "            # print(pred)\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    # Save last model\n",
        "    # if epoch == 10000 :\n",
        "    #     print(f'train_loop(): epoch {epoch} -- loss jumped from {prevLoss:.3} to {loss.item():.3}')\n",
        "    #     torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{epoch}_{batch}_{loss.item():.3}.pt')\n",
        "\n",
        "    train_loss /= size    # weighted avg training loss\n",
        "    train_accuracy /= size\n",
        "    print(f\"Training Error: \\n Accuracy: {(100*train_accuracy):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
        "    return train_loss, train_accuracy, confusion_matrix\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    # print(f'test dataset size: {size}')\n",
        "    # print(f'test num_batches: {num_batches}')\n",
        "    test_loss, test_accuracy = 0, 0\n",
        "    confusion_matrix = torch.zeros(20, 20, dtype=torch.int32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            # print(pred)\n",
        "            # print(y)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # y.dtype ensures the the operand types match for use w/ comparison operator (sensitive to type)\n",
        "            test_accuracy += (pred.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n",
        "            # Confusion matrix\n",
        "            for i in range(len(y)) :\n",
        "              preds = pred.argmax(1).type(y.dtype)\n",
        "              confusion_matrix[y[i]][preds[i]] += 1\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    test_accuracy /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*test_accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    # return test accuracy percentage for epoch\n",
        "    return test_accuracy, confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydc9rlm86Hmt"
      },
      "source": [
        "####Animator (d2l)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KnHcBLdIJYH"
      },
      "source": [
        "def use_svg_display():\n",
        "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
        "    display.set_matplotlib_formats('svg')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKViWEWBIK2E"
      },
      "source": [
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
        "    axes.set_xlabel(xlabel)\n",
        "    axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale)\n",
        "    axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim)\n",
        "    axes.set_ylim(ylim)\n",
        "    if legend:\n",
        "        axes.legend(legend)\n",
        "    axes.grid()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM5q0mh96LMw"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "class Animator:\n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
        "                 figsize=(3.5, 2.5)):\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "      \n",
        "        self.nrows = nrows\n",
        "        self.ncols = ncols\n",
        "        self.figsize = figsize\n",
        "\n",
        "        self.xlabel = xlabel\n",
        "        self.ylabel = ylabel\n",
        "        self.xlim = xlim\n",
        "        self.ylim = ylim\n",
        "        self.xscale = xscale\n",
        "        self.yscale = yscale\n",
        "        self.legend = legend\n",
        "\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "\n",
        "    \n",
        "    def display_plt(self):\n",
        "        # Borrowed use_svg_display() implementation from d2l\n",
        "        use_svg_display()\n",
        "        # matplot function\n",
        "        self.fig, self.axes = plt.subplots(self.nrows, self.ncols, figsize=self.figsize)\n",
        "        if self.nrows * self.ncols == 1:\n",
        "            self.axes = [self.axes,]\n",
        "        # Use a lambda function to capture arguments; set_axes in d2l API\n",
        "        self.config_axes = lambda: set_axes(self.axes[0],\n",
        "                                            self.xlabel,\n",
        "                                            self.ylabel,\n",
        "                                            self.xlim,\n",
        "                                            self.ylim,\n",
        "                                            self.xscale, self.yscale, self.legend)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "        # return self.fig"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ocO0Om3ofeX"
      },
      "source": [
        "####Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxqpHHcoomED"
      },
      "source": [
        "# %run /content/drive/MyDrive/'Colab Notebooks'/'Water Distribution Network'/'Input Pipeline'/'Data To File'/SimData_to_csv(hdf)_v1.ipynb\n",
        "class Conf_Mat() :\n",
        "  def __init__(self, classes=10) :\n",
        "    \"\"\"classes (int): the number of classes in the classifier.\"\"\"\n",
        "    self.confusion_matrix = torch.zeros(classes, classes, dtype=torch.int32)\n",
        "    # Load labels via JSON or csv file.\n",
        "    self.labels_str = ['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P20', 'P40', 'P1024']\n",
        "  \n",
        "  def addValues(self, pred, y) :\n",
        "    \"\"\"Add values to the confusion matrix.\n",
        "    pred (tensor): tensor containing model predictions.\n",
        "    y (tensor): tensor containing ground truth labels.\n",
        "    return None\n",
        "    \"\"\"\n",
        "    # Confusion Matrix\n",
        "    for i in range(len(y)) :\n",
        "      preds = pred.argmax(1).type(y.dtype)\n",
        "      confusion_matrix[y[i]][preds[i]] += 1\n",
        "    print(confusion_matrix)\n",
        "  \n",
        "  def displayConfMat(self) :\n",
        "    pass\n",
        "\n",
        "def decode_labels() :\n",
        "  # Time consuming\n",
        "  # Might be easier to place the encoder in a file and read it here.\n",
        "  # WARNING: lab_subset order is not consistent.\n",
        "  # NOTE: sets are not subscriptable\n",
        "  # net_charac, dir_path, simdata_dir, base_file, base_jfile, simdata_dir, dest_dir, set_size, time_stamp, debug = feat_lab_args()\n",
        "  # encoded_targets, lab_subset = labels_ds(base_jfile, simdata_dir, set_size, debug)\n",
        "  return sorted(['P1', 'P10', 'P100', 'P1000', 'P101', 'P1016', 'P102', 'P20', 'P40', 'P1024',\n",
        "                 'P39', 'P11', 'P800', 'P810', 'P446', 'P320', 'P981', 'P967', 'P942', 'P958',])\n",
        "# print(decode_labels())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVZsPo1p2RoP"
      },
      "source": [
        "####Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDXCstVB2XIb"
      },
      "source": [
        "def norm(features) :\n",
        "  # print(f'norm(): {features} size {features.size()}')\n",
        "  sq_feat = features ** 2\n",
        "  # print(f'norm(): {sq_feat} size {sq_feat.size()}')\n",
        "  sum_feat = sq_feat.sum(1)\n",
        "  # print(f'norm(): {sum_feat} size {sum_feat.size()}')\n",
        "  norm_feat = torch.sqrt(sum_feat)\n",
        "  # print(f'norm(): {norm_feat} size {norm_feat.size()}')\n",
        "  # print(norm_feat.view(features.size(0), 1))\n",
        "  unit_feat = features / norm_feat.view(features.size(0), 1)\n",
        "  # print(f'norm(): {unit_feat} size {unit_feat.size()}')\n",
        "  return unit_feat\n",
        "# _, observed, __ = SimData(net_char=4, tmstp=80)\n",
        "# norm(observed)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmFOoA-k258k"
      },
      "source": [
        "####Subsampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wCj-uKc561c"
      },
      "source": [
        "#####Mask Generation\n",
        "- (1/0) Sensing Mask Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZnE0eEEGO88"
      },
      "source": [
        "def sensing_mask_rand(feature_vec, max_sense = 30):\n",
        "  \"\"\"\n",
        "  preconditions: feature tensor must be flat.\n",
        "\n",
        "  Parameters\n",
        "  feature_vec: feature tensor to be masked\n",
        "   shape: list of dimensions\n",
        "  max_sense: randomly choose 1 to max_sense to be 1, rest 0\n",
        "\n",
        "  returns 0/1 sensing mask tensor of shape feature_vec w/ max_sense elems set to 1 (rest 0)\n",
        "  \"\"\"\n",
        "\n",
        "  # Create (0/1) mask populated w/ max_sense ones\n",
        "  mask = torch.zeros(feature_vec.size()).to(device)\n",
        "  # print(mask)\n",
        "  # indices = torch.randint(len(feature_vec), size=(max_sense,))   # may contain fewer than max_sense unique indices\n",
        "  # Katie: Randomize indices and choose the first max_sense\n",
        "  indices = [*range(len(feature_vec))]\n",
        "  random.shuffle(indices)\n",
        "  # print(indices)\n",
        "  # for idx in indices:   # for use with line 16\n",
        "  for idx in indices[:max_sense]:\n",
        "    mask[idx] = 1\n",
        "  # print(mask)\n",
        "\n",
        "  # Pass features through the mask\n",
        "  masked_features = feature_vec * mask\n",
        "\n",
        "  return mask, masked_features.to(device)\n",
        "# feature_vec = torch.rand([100])\n",
        "# print(feature_vec)\n",
        "# mask, masked_feats = sensing_mask_rand(feature_vec)\n",
        "# print(mask, '\\n', masked_feats)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaJ04dzFpbzL"
      },
      "source": [
        "def sensing_mask_alternate(feature_vec):\n",
        "  \"\"\"\n",
        "  preconditions: feature tensor must be flat.\n",
        "\n",
        "  Parameters\n",
        "  feature_vec: feature tensor to be masked\n",
        "\n",
        "  returns 0/1 sensing mask tensor of shape feature_vec w/ even indexed elems set to 1 (rest 0)\n",
        "  \"\"\"\n",
        "\n",
        "  # Create (0/1) mask populated w/ max_sense ones\n",
        "  mask = torch.zeros(feature_vec.size()).to(device)\n",
        "  # print(mask)\n",
        "  for idx in range(0, feature_vec.size()[0], 2) :\n",
        "    mask[idx] = 1\n",
        "  # print(mask)\n",
        "\n",
        "  # Pass features through the mask\n",
        "  # masked_features = feature_vec * mask\n",
        "\n",
        "  # Simplification: This version reduces the size of the feature\n",
        "  #  vector by half (mask not concat).\n",
        "  masked_features = feature_vec[0].reshape([1])\n",
        "  for idx in range(2, feature_vec.size()[0], 2) :\n",
        "    masked_features = torch.cat((masked_features, feature_vec[idx].reshape([1])))\n",
        "\n",
        "  return mask, masked_features.to(device)\n",
        "# feature_vec = torch.rand([100]).to(device)\n",
        "# print(feature_vec)\n",
        "# mask, masked_feats = sensing_mask_alternate(feature_vec)\n",
        "# print(mask, '\\n', masked_feats.size(), '\\n', masked_feats)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KkFT5x93F5H"
      },
      "source": [
        "#####Random subsamples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfeD3clN3QKu"
      },
      "source": [
        "def random_subset(meas_tensor, size, seed=None) :\n",
        "  \"\"\"Random subset of measurements (fixed seed).\n",
        "  Must be able to change the cardinality of the subset.\n",
        "  Algorithm\n",
        "    create list of indices\n",
        "    randomize indices\n",
        "    select first x number of indices (where x is the subset cardinality)\n",
        "    form a tensor containing the the sample measurements matching those indices.\n",
        "    return this tensor\n",
        "  \"\"\"\n",
        "  gen = None\n",
        "  if seed :\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "\n",
        "  rand_idxs = torch.randperm(meas_tensor.size()[0], generator=gen)\n",
        "  sub_idxs = rand_idxs[:size]\n",
        "  # print('random_subset():', sub_idxs, sub_idxs.size())\n",
        "  subset = meas_tensor[sub_idxs]\n",
        "  # print(meas_tensor.size())\n",
        "  return subset.to(device), sub_idxs\n",
        "\n",
        "# Test code\n",
        "# X = torch.rand(100)\n",
        "# rand_sub = random_subset(X, 50, 1000)\n",
        "# print('random_subset results:', rand_sub)\n",
        "# print(rand_sub.size())\n",
        "# assert False"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di_I5E4g4PDR"
      },
      "source": [
        "def rand_sub_dataset(X, size, seed=None) :\n",
        "  reduced_meas_X = torch.zeros([X.size(0), size]).to(device)\n",
        "  # print(X.size(0))\n",
        "  for i in range(len(X)) :\n",
        "    rand_sub, sub_idxs = random_subset(X[i], size, seed)\n",
        "    # print('random_subset results:', rand_sub)\n",
        "    # print(rand_sub.size())\n",
        "    reduced_meas_X[i] = rand_sub\n",
        "  return reduced_meas_X, sub_idxs\n",
        "\n",
        "# Test Code\n",
        "# X = torch.rand([3, 20])\n",
        "# size, seed = 10, 1000\n",
        "# reduced_meas_X, sub_idxs = rand_sub_dataset(X, size, seed)\n",
        "# # print(sub_idxs)\n",
        "# for i, idx in enumerate(sub_idxs) :\n",
        "#   # print(reduced_meas_X[0, i], X[0, idx])\n",
        "#   if reduced_meas_X[0, i].item() != X[0, idx].item() :\n",
        "#     raise Exception('rand_sub_dataset(): Error: Value doesnt match.')\n",
        "# # print(X[0, 307], X[0, 536], X[0, 329])\n",
        "# # print(reduced_meas_X[0, 0], reduced_meas_X[0, 1], reduced_meas_X[0, 2])\n",
        "# print('Done!')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if63qNQuS0HA"
      },
      "source": [
        "####Load SimData and form concat dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY7IINhjLGSQ"
      },
      "source": [
        "def SimData(net_char, tmstp) :\n",
        "  \"\"\"\n",
        "  Note: Dataset file generated in SimData_to_csv notebook\n",
        "   row 0: base_case\n",
        "   rows 1->set_size: observed; one leak scenario per row; 1hr (out of one week)\n",
        "   Labels included (last column)\n",
        "  \"\"\"\n",
        "\n",
        "  # Load dataframe\n",
        "  src_dir = '/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Input Pipeline/Datasets/'\n",
        "  leak_pip_ct = 20\n",
        "  leak_pipes = f'leak_pipes_{leak_pip_ct}/'\n",
        "  if tmstp :\n",
        "    ts_dir = f'tmstp{tmstp}/'\n",
        "  else :\n",
        "    ts_dir = ''\n",
        "  # NOTE: double check dataset file name\n",
        "  if net_char == 0:   dataset_file = 'dataset1000_link_flowrate_area0.01_0.1.csv'\n",
        "  elif net_char == 1: dataset_file = 'dataset1000_link_headloss_area0.01_0.1.cs'\n",
        "  elif net_char == 2: dataset_file = 'dataset1000_link_velocity_area0.01_0.1.cs'\n",
        "  elif net_char == 3: dataset_file = 'dataset1000_node_demand_area0.01_0.1.cs'\n",
        "  elif net_char == 4: dataset_file = 'dataset1000_node_head_area0.01_0.1.csv'\n",
        "  elif net_char == 5: dataset_file = 'dataset1000_node_pressure_area0.01_0.1.csv'\n",
        "  \n",
        "  data_file = src_dir + leak_pipes + ts_dir + dataset_file\n",
        "  datast_ds = pd.read_csv(data_file)\n",
        "  # print(datast_ds.head())\n",
        "  \n",
        "  # Separate base_case, raw_data, and encoded labels\n",
        "  # Base Case\n",
        "  base_case = torch.tensor(datast_ds.values[0, :-1], dtype=torch.float32).to(device)\n",
        "  # print(base_case)\n",
        "  # raw_data\n",
        "  raw_data = torch.tensor(datast_ds.values[1: , :-1], dtype=torch.float32).to(device)\n",
        "  print(f'SimData(): raw_data {raw_data.size()}')\n",
        "  # encoded labels\n",
        "  labels = torch.tensor(datast_ds['Label'], dtype=torch.long)[1:].to(device)\n",
        "  print(f'SimData(): labels {labels.size()}')\n",
        "\n",
        "  return base_case, raw_data, labels\n",
        "# _, __, ___ = SimData()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6Xn9Due5kYf"
      },
      "source": [
        "def leakpipe_subset(pipe_ct, raw, enc_labs):\n",
        "  \"Used to limit the number of pipes in the dataset\"\n",
        "  tmp_raw = torch.tensor([[0]]).to(device)   # dim trouble w/ cat\n",
        "  tmp_labs = torch.tensor([0]).to(device)\n",
        "  print(tmp_labs.size())\n",
        "\n",
        "  for i in range(len(enc_labs)):\n",
        "    if enc_labs[i] < pipe_ct:\n",
        "      tmp_raw = torch.cat((tmp_raw, raw[i].reshape([444])))\n",
        "      tmp_labs = torch.cat((tmp_labs, enc_labs[i].reshape([1])))\n",
        "\n",
        "  return tmp_raw[1:], tmp_labs[1:]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alN3uGMtzKbn"
      },
      "source": [
        "def cat_net_attr(net_ls, tmstp, norm_feats=False) :\n",
        "  cat_attrs = None\n",
        "  for net_char in net_ls :\n",
        "    # base_case, X_raw, encoded_labels\n",
        "    data = SimData(net_char, tmstp)\n",
        "\n",
        "    if not cat_attrs :\n",
        "      # cat_attr = base_case, X_raw, encoded_labels\n",
        "      cat_attrs = data\n",
        "      cat_attrs = list(cat_attrs)   # Creates 3 elem list from data 3-tuple returned by SimData.\n",
        "      if norm_feats :\n",
        "        cat_attrs[1] = norm(data[1])\n",
        "    else :\n",
        "      # for i in range(0, len(cat_attr), 2) :\n",
        "      #   cat_attr[i] = torch.cat((cat_attr[i], data[i]))\n",
        "      cat_attrs[0] = torch.cat((cat_attrs[0], data[0]))\n",
        "      if norm_feats :\n",
        "        cat_attrs[1] = torch.cat((cat_attrs[1], norm(data[1])), dim=1)\n",
        "      else :\n",
        "        cat_attrs[1] = torch.cat((cat_attrs[1], data[1]), dim=1)\n",
        "      # only need one set of label; no need to cat those again.\n",
        "    # Normalize Features\n",
        "    if norm_feats:\n",
        "      # Raw data normed across all samples\n",
        "      #  Would it make more sense to normalize ea sample individually?\n",
        "      # X_raw = torch.nn.functional.normalize(X_raw, dim=0)\n",
        "      cat_attrs[1] = norm(cat_attrs[1])\n",
        "    # print(f'cat_data(): {norm_str}expected {expected.size()}')\n",
        "    # print(f'cat_data(): {norm_str}expected {expected}')\n",
        "  # print(cat_attrs)\n",
        "  print(cat_attrs[0].size(), cat_attrs[1].size(), cat_attrs[2].size())\n",
        "  return cat_attrs   # cat_attrs is a list, but can be expanded by the caller.\n",
        "# _, __, ___ = cat_net_attr([0, 4], 80, True)\n",
        "# print(_.size(), __.size(), ___.size())\n",
        "# assert False"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWwFaq5FMiPh"
      },
      "source": [
        "def cat_data(residual=False, norm_base=False, norm_feats=False, mask=False, net_char=[0], tmstp=None) :\n",
        "  if isinstance(net_char, int) :\n",
        "    net_char = [net_char]\n",
        "  expected, X_raw, encoded_labels = cat_net_attr(net_char, tmstp)\n",
        "  # expected, X_raw, encoded_labels = SimData(net_char, tmstp)\n",
        "  # print(f'cat_data(): {expected}')\n",
        "  print(f'cat_data(): X_raw max {torch.max(X_raw)}, min {torch.min(X_raw)}')\n",
        "  print(f'cat_data(): X_raw range {torch.max(X_raw) + abs(torch.min(X_raw))}')\n",
        "  print(f'cat_data(): X_raw {X_raw.size()}')\n",
        "  # print(f'cat_data(): X_raw {X_raw[0]}')\n",
        "  # print(f'cat_data(): expected {expected}')\n",
        "  \n",
        "  # work in progress\n",
        "  # X_raw, encoded_labels = leakpipe_subset(10, X_raw, encoded_labels)\n",
        "  # print(f'cat_data: encoded labels \\n{encoded_labels}')\n",
        "  # print(f'cat_data: encoded labels {encoded_labels.size()}')\n",
        "  # assert False\n",
        "\n",
        "  X_cat = None\n",
        "  norm_str = ''   # Used for debug output\n",
        "  div_by_exp = expected\n",
        "  # Residual\n",
        "  if residual:\n",
        "    # Avoid dividing by zero\n",
        "    # for idx in range(len(expected)):\n",
        "      # if expected[idx] < 1.0e-12:\n",
        "      #   div_by_exp[idx] = 1.0e-10\n",
        "    # X_raw = X_raw / div_by_exp\n",
        "    X_raw -= expected\n",
        "    # expected /= expected   # might divide by zero\n",
        "    # pass\n",
        "  # print(f'cat_data(): X_raw {X_raw[0]}')\n",
        "  \n",
        "  # Normalizations\n",
        "  if norm_base:\n",
        "    # Normalize expected\n",
        "    #  orders of magnitude larger than residual data\n",
        "    #  v = v / max(||v||_p, epsilon)  where epsilon is a small value that void dividing by zero\n",
        "    # norm_expected = torch.nn.functional.normalize(expected.reshape([1,-1]))\n",
        "    expected = torch.nn.functional.normalize(expected, dim=0) * 10.0\n",
        "    norm_str = 'norm_'\n",
        "  # if norm_feats:\n",
        "  #   # Raw data normed across all samples\n",
        "  #   #  Would it make more sense to normalize ea sample individually?\n",
        "  #   # X_raw = torch.nn.functional.normalize(X_raw, dim=0)\n",
        "  #   X_raw = norm(X_raw)\n",
        "  # print(f'cat_data(): {norm_str}expected {expected.size()}')\n",
        "  # # print(f'cat_data(): {norm_str}expected {expected}')\n",
        "\n",
        "  # I think I can move if mask outter and elim the for loop.\n",
        "  for feature_vec in X_raw:\n",
        "    if mask :\n",
        "      # Mask and masked features\n",
        "      #  May want sensing_mask_rand() that can process batches of samples\n",
        "      # mask_tn, masked_feats = sensing_mask_rand(feature_vec)\n",
        "      # Construct feature set from concatination of expected, mask, and masked measuremnts.\n",
        "      # temp = torch.cat((masked_feats.to(device), mask.to(device), expected.to(device))).reshape([1,-1])\n",
        "      # Expected measurements not included\n",
        "      # temp = torch.cat((masked_feats.to(device), mask_tn.to(device))).reshape([1, -1])\n",
        "      # Simplification: notice feature vector size is halved. Update col variable accordingly.\n",
        "      mask_tn, masked_feats = sensing_mask_alternate(feature_vec)\n",
        "      # Not Needed. Adjust size of label vector.\n",
        "      # masked_labels = encoded_labels[0].reshape([1])\n",
        "      # print(encoded_labels.size())\n",
        "      # assert False\n",
        "      # for idx in range(2, encoded_labels.size()[0], 2) :\n",
        "      #   masked_labels = torch.cat((masked_labels, encoded_labels[idx].reshape([1])))\n",
        "      temp = masked_feats.reshape([1,-1])\n",
        "      # encoded_labels = masked_labels\n",
        "      # print(f'cat_data(), mask: enc_lab size {encoded_labels.size()}')\n",
        "\n",
        "    else :\n",
        "      # Features (no mask)\n",
        "      # Construct feature set from concatination of expected and observed measuremnts.\n",
        "      # temp = torch.cat((feature_vec.to(device), expected.to(device))).reshape([1,-1])\n",
        "      temp = feature_vec.reshape([1,-1])\n",
        "    # print(f'cat_data(): temp {temp}')\n",
        "\n",
        "    if X_cat is None:\n",
        "      X_cat = temp\n",
        "      # print(X_cat)\n",
        "    else:  \n",
        "      X_cat = torch.cat((X_cat, temp))\n",
        "      # print(X_cat)\n",
        "\n",
        "  print(f'cat_data(): X_cat {X_cat.size()}')\n",
        "  # print(f'cat_data(): X_cat {X_cat[0]}')\n",
        "  # assert False\n",
        "  return X_cat, encoded_labels\n",
        "# print(f'output: {cat_data(residual=True, norm_exp=False, norm_feats=False, mask=False)}')\n",
        "# assert False"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdawEj-SS9hP"
      },
      "source": [
        "####Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Zwmb6sYpbm"
      },
      "source": [
        "#####Set-up Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLehjrJ5SV8M"
      },
      "source": [
        "rows = 0\n",
        "cols = 0\n",
        "input_dim = 0\n",
        "output_dim = 0\n",
        "\n",
        "tr_dataset = None\n",
        "ts_dataset = None"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJJN6lECYvM_"
      },
      "source": [
        "from collections import Counter\n",
        "import statistics as stats\n",
        "\n",
        "def histo_pipe_dist(labels) :\n",
        "  \"\"\"Display histogram of leakpipe distribution.\n",
        "  labels (tensor) : pytorch tensor of labels (ints).\n",
        "  Note: requires matplotlib and Pandas.\n",
        "  \"\"\"\n",
        "  # Histo\n",
        "  Y = pd.Series(labels.cpu())\n",
        "  recounted = Counter(Y)\n",
        "  print(recounted)\n",
        "  std = stats.stdev(recounted.values())\n",
        "  print(f'stdev {std:.2}')\n",
        "  Y.plot.hist(grid=True, bins=10, alpha=0.7, rwidth=0.8, color='#607c8e', align='mid')\n",
        "  plt.title(f'Label Frequency for {len(Y)} Samples')\n",
        "  plt.xlabel('Label')\n",
        "  plt.grid(axis='x')\n",
        "  # plt.text(6, 200, r'class 5 = 229 (46%)')\n",
        "  # assert False\n",
        "  return std"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpisWMQEGjxL"
      },
      "source": [
        "def randomize_dataset(X, y) :\n",
        "  random.seed(10343)\n",
        "  ls = []\n",
        "  for i in range(len(y)) :\n",
        "    zipped = X[i], y[i]\n",
        "    # print(zipped, end=\" \")\n",
        "    ls.append(zipped)\n",
        "  # print('randomize_dataset(): ls', len(ls))\n",
        "  random.shuffle(ls)\n",
        "  shuf_X = torch.empty([1, len(X[0])]).to(device)\n",
        "  # print(shuf_X.size(), shuf_X)\n",
        "  shuf_y = torch.empty([1], dtype=torch.long).to(device)\n",
        "  # print(shuf_X.size(), shuf_X)\n",
        "  for feat, label in ls :\n",
        "    # print('randomize_dataset(): feat', feat.reshape([1, -1]).size())\n",
        "    shuf_X = torch.cat((shuf_X, feat.reshape([1, -1])))\n",
        "    # print('randomize_dataset(): label', label.reshape([1]).size())\n",
        "    # label = torch.tensor(label, dtype=torch.long).to(device)\n",
        "    shuf_y = torch.cat( (shuf_y, label.reshape([1])) )\n",
        "  # print(shuf_X)\n",
        "  # print(shuf_y)\n",
        "  # print('randomize_dataset(): shuf_X', shuf_X.size())\n",
        "  # print('randomize_dataset(): shuf_y', shuf_y.size())\n",
        "  return shuf_X[1:], shuf_y[1:]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfhI0Do7Ad_o"
      },
      "source": [
        "def find_seed(X, y, tr_size, tr_or_ts) :\n",
        "  min_std = 100\n",
        "  # tr_or_ts = 0  # tr = 0, ts = 1\n",
        "  for i in range(20, 150) :\n",
        "    print(i)\n",
        "    subsets = torch.utils.data.random_split(TensorDataset(X, y),\n",
        "                                            [tr_size, len(y) - tr_size],\n",
        "                                            generator=torch.Generator().manual_seed(i),\n",
        "                                            )\n",
        "    # print(len(subsets),\n",
        "    #       subsets[1].dataset.tensors[1].size(),\n",
        "    #       type(subsets[1].indices),\n",
        "    #       len(subsets[1].indices),\n",
        "    #       subsets[1].dataset.tensors[1][[i for i in subsets[1].indices]],\n",
        "    #       )\n",
        "    std = histo_pipe_dist(subsets[tr_or_ts].dataset.tensors[1][[i for i in subsets[tr_or_ts].indices]])\n",
        "    if std < min_std :\n",
        "      min_std = std\n",
        "      seed = i\n",
        "  print(seed)\n",
        "  return seed"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qysp3xV5W2dg"
      },
      "source": [
        "#####MNISTfashion set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "v2G9BTikRI0k",
        "outputId": "ba1711b3-b874-456d-853d-445319c42c5e"
      },
      "source": [
        "# mnist\n",
        "\"\"\"\n",
        "tr_dataset = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "ts_dataset = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "rows = 28\n",
        "cols = 28\n",
        "input_dim = rows*cols\n",
        "output_dim = 10\n",
        "\n",
        "learning_rate = 1e-3\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "mod = 100\n",
        "#\"\"\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntr_dataset = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=True,\\n    download=True,\\n    transform=transforms.ToTensor()\\n)\\n\\nts_dataset = datasets.FashionMNIST(\\n    root=\"data\",\\n    train=False,\\n    download=True,\\n    transform=transforms.ToTensor()\\n)\\n\\nrows = 28\\ncols = 28\\ninput_dim = rows*cols\\noutput_dim = 10\\n\\nlearning_rate = 1e-3\\nepochs = 10\\nbatch_size = 64\\nmod = 100\\n#'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBUCPLjMROO0"
      },
      "source": [
        "#####SimData"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sPNhrjauC1OW",
        "outputId": "3f494cd3-ed13-4f54-8f61-7dedf196d754"
      },
      "source": [
        "# Network characterist options:\n",
        "#  {0: '/link_flowrate', 1: '/link_headloss', 2: '/link_velocity',\n",
        "#   3: '/node_demand', 4: '/node_head', 5: '/node_pressure'}\n",
        "residual = True\n",
        "norm_exp = False\n",
        "norm_feats = True\n",
        "mask = False\n",
        "net_char = 0\n",
        "tmstp = 80\n",
        "cat_attrs = True\n",
        "subsample = True\n",
        "if cat_attrs :\n",
        "  net_char = [0, 4]\n",
        "X, y = cat_data(residual, norm_exp, norm_feats, mask, net_char, tmstp)\n",
        "# print(y)\n",
        "torch.set_printoptions(edgeitems=50)\n",
        "# print(f'Training Set: X[0] {X[0]}')\n",
        "\n",
        "# Visualize the dataset leak pipe distribution.\n",
        "# histo_pipe_dist(y)\n",
        "\n",
        "if subsample :\n",
        "  perc_of_meas = 0.01\n",
        "  size = int(X.size(1) * perc_of_meas)\n",
        "  print('subsample: size ', size)\n",
        "  seed = 1001\n",
        "  reduced_meas_X, __ = rand_sub_dataset(X, size, seed)\n",
        "  print('subsample:', reduced_meas_X.size())\n",
        "  # print(X[0, 307], X[0, 536], X[0, 329])\n",
        "  # print(reduced_meas_X[0, 0], reduced_meas_X[0, 1], reduced_meas_X[0, 2])\n",
        "  X = reduced_meas_X\n",
        "\n",
        "torch.set_printoptions(edgeitems=3)\n",
        "tr_size = int(len(X)*0.7)\n",
        "ts_size = len(y) - tr_size\n",
        "# tr_dataset = TensorDataset(X[:split_idx], y[:split_idx])\n",
        "# ts_dataset = TensorDataset(X[split_idx:], y[split_idx:])\n",
        "# Find a seed that creates a training set pipe distribution with smallest stdev.\n",
        "seed = 135\n",
        "# seed = find_seed(X, y, tr_size, 0)\n",
        "print(f'chosen seed = {seed}')\n",
        "# assert False\n",
        "tr_dataset, ts_dataset = torch.utils.data.random_split(TensorDataset(X, y),\n",
        "                                        [tr_size, ts_size],\n",
        "                                        generator=torch.Generator().manual_seed(seed),\n",
        "                                        )\n",
        "\n",
        "# X, y = randomize_dataset(X, y)\n",
        "# histo_pipe_dist(y[:split_idx])\n",
        "# histo_pipe_dist(y[split_idx:])\n",
        "idx = tr_dataset.indices\n",
        "histo_pipe_dist(tr_dataset.dataset.tensors[1][[i for i in idx]])\n",
        "\n",
        "# Determine learning rate and measurement vector size\n",
        "if net_char == 0:\n",
        "  # link_flowrate\n",
        "  cols = 444   # links = 444; junctions = 396\n",
        "  # learning_rate = 9e-3   # single layer\n",
        "  learning_rate = 8e-2\n",
        "if net_char == 2:\n",
        "  # link_velocity\n",
        "  cols = 444   # links = 444; junctions = 396\n",
        "  learning_rate = 2e-7\n",
        "elif net_char == 3:\n",
        "  # node_demand\n",
        "  cols = 396   # links = 444; junctions = 396\n",
        "  learning_rate = 2e-6\n",
        "  epochs = 2000\n",
        "elif net_char == 4:\n",
        "  # node_head\n",
        "  cols = 396   # links = 444; junctions = 396\n",
        "  learning_rate = 7e-2\n",
        "  epochs = 2000\n",
        "elif net_char == 5:\n",
        "  # node_pressure\n",
        "  cols = 396   # links = 444; junctions = 396\n",
        "  learning_rate = 1e-3\n",
        "elif isinstance(net_char, list) :\n",
        "  if norm_feats and cat_attrs :\n",
        "    learning_rate = 2e-2\n",
        "  else :\n",
        "    learning_rate = 8e-2   #(f, h, nf, nh, f+h)\n",
        "\n",
        "# Determine number of concatenated vectors. Used for determining input_dim\n",
        "concats = 1\n",
        "if mask :\n",
        "  concats = 1\n",
        "\n",
        "elif norm_feats :\n",
        "  pass\n",
        "# Simplification adjustments\n",
        "cols = X.size(1)\n",
        "print(f'cols {cols}')\n",
        "\n",
        "\n",
        "rows = 1\n",
        "input_dim = rows*cols*concats\n",
        "output_dim = 20\n",
        "epochs = 20000\n",
        "batch_size = 128\n",
        "mod = 5\n",
        "\n",
        "# Instantiate model framework\n",
        "model = Decoder(input_dim, output_dim).to(device)\n",
        "print(model)\n",
        "# print(*model.parameters())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimData(): raw_data torch.Size([1000, 444])\n",
            "SimData(): labels torch.Size([1000])\n",
            "SimData(): raw_data torch.Size([1000, 396])\n",
            "SimData(): labels torch.Size([1000])\n",
            "torch.Size([840]) torch.Size([1000, 840]) torch.Size([1000])\n",
            "cat_data(): X_raw max 152.2197723388672, min -0.19309654831886292\n",
            "cat_data(): X_raw range 152.41287231445312\n",
            "cat_data(): X_raw torch.Size([1000, 840])\n",
            "cat_data(): X_cat torch.Size([1000, 840])\n",
            "subsample: size  8\n",
            "subsample: torch.Size([1000, 8])\n",
            "chosen seed = 135\n",
            "Counter({5: 46, 18: 41, 1: 39, 14: 37, 11: 37, 10: 37, 0: 36, 8: 35, 12: 35, 2: 35, 13: 35, 3: 35, 9: 34, 4: 33, 17: 33, 7: 31, 15: 31, 19: 31, 16: 30, 6: 29})\n",
            "stdev 4.0\n",
            "cols 8\n",
            "Decoder(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_lrelu_stack): Sequential(\n",
            "    (0): Linear(in_features=8, out_features=512, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Linear(in_features=512, out_features=20, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.01)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQdyRZRYXiQJ"
      },
      "source": [
        "#####Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LItPDTO9LCoY",
        "outputId": "365a2d48-ef1e-4647-ab16-7a63d933fd82"
      },
      "source": [
        "# Initialize the loss function\n",
        "#  nn.CrossEntropyLoss() encapsulates nn.LogSoftmax and nn.NLLLoss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Parameter adjustment protocol\n",
        "# He: always start w/ Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# For plotting results\n",
        "animator = Animator(xlabel='epoch', xlim=[1, epochs], ylim=[0.0, 1.0],\n",
        "                        legend=['train loss', 'train acc', 'test acc'])\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_dataloader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # for x,y in train_dataloader:\n",
        "    #   print(x,y)\n",
        "    #   break\n",
        "    tr_loss, __, _ConfM_ = train_loop(train_dataloader, model, loss_fn, optimizer, epoch=t+1, mod=mod)\n",
        "    train_metrics = (tr_loss, __)\n",
        "\n",
        "    # Disaggregate data -- save models; note epoch\n",
        "    # if t > 2000 and tr_loss > prevLoss * 10 :\n",
        "    #   print(f'train_loop(): epoch {t} -- loss jumped from {prevLoss:.3} to {tr_loss:.3}')\n",
        "    #   torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Water Distribution Network/Decoder/saved_models/d11_epoch{t}.pt')\n",
        "    # prevLoss = tr_loss\n",
        "\n",
        "    test_dataloader = DataLoader(ts_dataset, batch_size=batch_size)\n",
        "    test_acc, confusion_matrix = test_loop(test_dataloader, model, loss_fn)\n",
        "    # animator\n",
        "    animator.add(t + 1, train_metrics + (test_acc,))\n",
        "# Not sure the following block is necessary\n",
        "# train_loss, train_acc = train_metrics\n",
        "# assert train_loss < 0.5, train_loss\n",
        "# assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
        "# assert test_acc <= 1 and test_acc > 0.7, test_acc\n",
        "print(\"Done!\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.982670  [    0/  700]\n",
            "loss: 2.778851  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 11.0%, Avg loss: 2.964360 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 15.0%, Avg loss: 2.798735 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.737840  [    0/  700]\n",
            "loss: 2.539894  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 15.7%, Avg loss: 2.711545 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 14.0%, Avg loss: 2.673264 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.473362  [    0/  700]\n",
            "loss: 2.514197  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 18.1%, Avg loss: 2.589089 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 15.3%, Avg loss: 2.613080 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.370465  [    0/  700]\n",
            "loss: 2.408095  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 17.9%, Avg loss: 2.524106 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 23.3%, Avg loss: 2.547852 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.323386  [    0/  700]\n",
            "loss: 2.387059  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 22.0%, Avg loss: 2.468659 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 22.0%, Avg loss: 2.524545 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.241622  [    0/  700]\n",
            "loss: 2.324498  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 23.9%, Avg loss: 2.431612 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 24.7%, Avg loss: 2.463810 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.167111  [    0/  700]\n",
            "loss: 2.285421  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 26.0%, Avg loss: 2.356916 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 21.7%, Avg loss: 2.445218 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.113367  [    0/  700]\n",
            "loss: 2.232135  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 26.0%, Avg loss: 2.326150 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 23.3%, Avg loss: 2.411508 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.068682  [    0/  700]\n",
            "loss: 2.208714  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 25.1%, Avg loss: 2.289036 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 22.3%, Avg loss: 2.384998 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.025061  [    0/  700]\n",
            "loss: 2.095246  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 26.3%, Avg loss: 2.251973 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 21.7%, Avg loss: 2.339268 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.985289  [    0/  700]\n",
            "loss: 2.120505  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 24.6%, Avg loss: 2.217065 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 22.3%, Avg loss: 2.337130 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.955025  [    0/  700]\n",
            "loss: 2.028774  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 26.9%, Avg loss: 2.182051 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 24.3%, Avg loss: 2.297960 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.927741  [    0/  700]\n",
            "loss: 2.020973  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 25.9%, Avg loss: 2.166398 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 22.3%, Avg loss: 2.301318 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.900603  [    0/  700]\n",
            "loss: 2.003549  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 26.9%, Avg loss: 2.152019 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 22.3%, Avg loss: 2.295753 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.888219  [    0/  700]\n",
            "loss: 1.989845  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 25.3%, Avg loss: 2.141686 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 24.0%, Avg loss: 2.279647 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.874560  [    0/  700]\n",
            "loss: 1.969802  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 29.4%, Avg loss: 2.126219 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 24.3%, Avg loss: 2.278478 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.865093  [    0/  700]\n",
            "loss: 1.991019  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 27.9%, Avg loss: 2.124451 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 24.0%, Avg loss: 2.275686 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.853924  [    0/  700]\n",
            "loss: 1.948764  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 29.3%, Avg loss: 2.110779 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 24.0%, Avg loss: 2.269888 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.843758  [    0/  700]\n",
            "loss: 1.970884  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 29.0%, Avg loss: 2.107280 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 24.0%, Avg loss: 2.265412 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.827922  [    0/  700]\n",
            "loss: 1.950818  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 30.9%, Avg loss: 2.074918 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.0%, Avg loss: 2.252491 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.803731  [    0/  700]\n",
            "loss: 1.987799  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 29.7%, Avg loss: 2.077941 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 24.3%, Avg loss: 2.264998 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.802113  [    0/  700]\n",
            "loss: 1.927347  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 30.1%, Avg loss: 2.068390 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 25.3%, Avg loss: 2.248463 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.785175  [    0/  700]\n",
            "loss: 1.887083  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 30.6%, Avg loss: 2.032919 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 24.7%, Avg loss: 2.159644 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 1.749511  [    0/  700]\n",
            "loss: 1.869698  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 30.1%, Avg loss: 2.003818 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 26.3%, Avg loss: 2.144794 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.748281  [    0/  700]\n",
            "loss: 1.844346  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 32.0%, Avg loss: 1.991175 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 27.3%, Avg loss: 2.135735 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 1.729912  [    0/  700]\n",
            "loss: 1.853950  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 32.6%, Avg loss: 1.983032 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.3%, Avg loss: 2.125785 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 1.726098  [    0/  700]\n",
            "loss: 1.828082  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.971670 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.7%, Avg loss: 2.127156 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 1.718620  [    0/  700]\n",
            "loss: 1.842049  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.967219 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 28.0%, Avg loss: 2.118132 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 1.712749  [    0/  700]\n",
            "loss: 1.818082  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.961154 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 2.111221 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 1.707403  [    0/  700]\n",
            "loss: 1.819969  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 33.3%, Avg loss: 1.953423 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 27.7%, Avg loss: 2.111180 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 1.703856  [    0/  700]\n",
            "loss: 1.813019  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 34.4%, Avg loss: 1.952796 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 2.101088 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 1.699322  [    0/  700]\n",
            "loss: 1.807878  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.945038 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 2.109560 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 1.695614  [    0/  700]\n",
            "loss: 1.797922  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.943199 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.7%, Avg loss: 2.097604 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 1.689730  [    0/  700]\n",
            "loss: 1.795799  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.934495 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.3%, Avg loss: 2.099470 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 1.687398  [    0/  700]\n",
            "loss: 1.790542  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 35.1%, Avg loss: 1.933104 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.7%, Avg loss: 2.093939 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 1.678052  [    0/  700]\n",
            "loss: 1.787108  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.928874 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 2.094228 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 1.685803  [    0/  700]\n",
            "loss: 1.781103  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.925525 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 2.097581 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 1.670477  [    0/  700]\n",
            "loss: 1.777643  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 35.4%, Avg loss: 1.922214 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 2.091899 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 1.679140  [    0/  700]\n",
            "loss: 1.772454  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.917318 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 29.3%, Avg loss: 2.089342 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 1.660736  [    0/  700]\n",
            "loss: 1.768802  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.912509 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 30.7%, Avg loss: 2.090859 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 1.671564  [    0/  700]\n",
            "loss: 1.764418  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.908447 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 2.085706 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 1.655514  [    0/  700]\n",
            "loss: 1.760613  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 35.9%, Avg loss: 1.907570 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 2.083388 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 1.665307  [    0/  700]\n",
            "loss: 1.756297  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.901850 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 2.082517 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 1.649103  [    0/  700]\n",
            "loss: 1.749863  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.897275 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 2.083584 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 1.657555  [    0/  700]\n",
            "loss: 1.745321  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.896735 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.0%, Avg loss: 2.074476 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 1.644347  [    0/  700]\n",
            "loss: 1.740745  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.890078 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 2.079154 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 1.653856  [    0/  700]\n",
            "loss: 1.743169  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.890116 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.3%, Avg loss: 2.076323 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 1.638379  [    0/  700]\n",
            "loss: 1.729506  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.886153 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.072335 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 1.646832  [    0/  700]\n",
            "loss: 1.740675  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 36.4%, Avg loss: 1.882446 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 2.072521 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 1.629575  [    0/  700]\n",
            "loss: 1.724632  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.877759 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 31.7%, Avg loss: 2.071658 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 1.645105  [    0/  700]\n",
            "loss: 1.725887  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.876434 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 2.073708 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 1.626206  [    0/  700]\n",
            "loss: 1.725611  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.872889 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.067230 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 1.638204  [    0/  700]\n",
            "loss: 1.711805  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.868647 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 2.067085 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 1.618111  [    0/  700]\n",
            "loss: 1.719524  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.865561 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.067274 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 1.634341  [    0/  700]\n",
            "loss: 1.702876  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.863065 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 2.060825 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 1.612164  [    0/  700]\n",
            "loss: 1.709035  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.858661 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 2.067828 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 1.633942  [    0/  700]\n",
            "loss: 1.702840  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.859201 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 2.064275 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 1.601612  [    0/  700]\n",
            "loss: 1.698981  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.854045 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 2.063063 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 1.633045  [    0/  700]\n",
            "loss: 1.705387  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.853523 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.061634 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 1.600239  [    0/  700]\n",
            "loss: 1.686216  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.1%, Avg loss: 1.846154 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 2.062761 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 1.633458  [    0/  700]\n",
            "loss: 1.691348  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.848598 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 2.062048 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 1.585117  [    0/  700]\n",
            "loss: 1.690923  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.841879 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 2.059940 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 1.635309  [    0/  700]\n",
            "loss: 1.679808  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.843782 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 2.054910 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 1.586642  [    0/  700]\n",
            "loss: 1.675943  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.835135 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.058333 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 1.635174  [    0/  700]\n",
            "loss: 1.668519  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.837909 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 2.056721 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 1.582870  [    0/  700]\n",
            "loss: 1.672698  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 38.3%, Avg loss: 1.830552 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.055733 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 1.631854  [    0/  700]\n",
            "loss: 1.666100  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.834862 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.054736 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 1.573903  [    0/  700]\n",
            "loss: 1.673462  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 37.9%, Avg loss: 1.827280 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 2.051807 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 1.629658  [    0/  700]\n",
            "loss: 1.660525  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.829179 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 2.044518 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 1.570766  [    0/  700]\n",
            "loss: 1.663017  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 38.4%, Avg loss: 1.820641 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.047870 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 1.632602  [    0/  700]\n",
            "loss: 1.655363  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.822509 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.047094 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 1.562573  [    0/  700]\n",
            "loss: 1.659006  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.816098 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.054399 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 1.627025  [    0/  700]\n",
            "loss: 1.655435  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.819102 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 2.044956 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 1.561637  [    0/  700]\n",
            "loss: 1.649986  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.812288 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.050359 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 1.631619  [    0/  700]\n",
            "loss: 1.644975  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.6%, Avg loss: 1.815764 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 2.042418 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 1.555853  [    0/  700]\n",
            "loss: 1.640465  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.804868 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.047788 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 1.622266  [    0/  700]\n",
            "loss: 1.631818  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.0%, Avg loss: 1.806764 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.038649 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 1.553749  [    0/  700]\n",
            "loss: 1.632495  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.9%, Avg loss: 1.798894 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.045748 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 1.621271  [    0/  700]\n",
            "loss: 1.621470  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.3%, Avg loss: 1.800233 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.041280 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 1.546122  [    0/  700]\n",
            "loss: 1.631905  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.794856 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.046404 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 1.606375  [    0/  700]\n",
            "loss: 1.620550  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.3%, Avg loss: 1.796013 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.037662 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 1.544643  [    0/  700]\n",
            "loss: 1.626417  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.4%, Avg loss: 1.792084 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.044190 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 1.609421  [    0/  700]\n",
            "loss: 1.615133  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.1%, Avg loss: 1.791535 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.038618 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 1.542004  [    0/  700]\n",
            "loss: 1.622957  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.4%, Avg loss: 1.784673 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.053048 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 1.593405  [    0/  700]\n",
            "loss: 1.608852  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.786126 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 2.036416 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 1.539690  [    0/  700]\n",
            "loss: 1.610748  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.3%, Avg loss: 1.781879 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 2.046269 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 1.592195  [    0/  700]\n",
            "loss: 1.610124  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.4%, Avg loss: 1.781678 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 2.035487 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 1.534144  [    0/  700]\n",
            "loss: 1.600201  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.777410 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.038333 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 1.593456  [    0/  700]\n",
            "loss: 1.599538  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.9%, Avg loss: 1.775607 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.0%, Avg loss: 2.042726 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 1.534549  [    0/  700]\n",
            "loss: 1.598164  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.0%, Avg loss: 1.773035 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.040508 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 1.583031  [    0/  700]\n",
            "loss: 1.594288  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.1%, Avg loss: 1.771301 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 2.034164 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 1.531499  [    0/  700]\n",
            "loss: 1.595269  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.766876 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.7%, Avg loss: 2.036213 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 1.567810  [    0/  700]\n",
            "loss: 1.583924  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.3%, Avg loss: 1.761203 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.029056 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 1.528103  [    0/  700]\n",
            "loss: 1.586430  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.0%, Avg loss: 1.760029 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 2.029015 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 1.566965  [    0/  700]\n",
            "loss: 1.577193  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.9%, Avg loss: 1.758809 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.0%, Avg loss: 2.027897 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 1.528654  [    0/  700]\n",
            "loss: 1.588489  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.1%, Avg loss: 1.757351 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.032051 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 1.553242  [    0/  700]\n",
            "loss: 1.576997  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.0%, Avg loss: 1.754621 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.024910 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 1.532249  [    0/  700]\n",
            "loss: 1.590014  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.750002 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 2.039282 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 1.556611  [    0/  700]\n",
            "loss: 1.588710  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.9%, Avg loss: 1.737667 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.019582 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 1.532874  [    0/  700]\n",
            "loss: 1.577721  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.4%, Avg loss: 1.732829 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.024833 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "loss: 1.552523  [    0/  700]\n",
            "loss: 1.567528  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.1%, Avg loss: 1.727822 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 2.017911 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "loss: 1.521258  [    0/  700]\n",
            "loss: 1.580222  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.6%, Avg loss: 1.723138 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 2.028867 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "loss: 1.543751  [    0/  700]\n",
            "loss: 1.564674  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.3%, Avg loss: 1.723739 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.015203 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "loss: 1.517484  [    0/  700]\n",
            "loss: 1.571399  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.4%, Avg loss: 1.716980 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.022199 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "loss: 1.544027  [    0/  700]\n",
            "loss: 1.568276  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.7%, Avg loss: 1.717113 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.019715 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "loss: 1.512784  [    0/  700]\n",
            "loss: 1.568810  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.7%, Avg loss: 1.714452 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 2.025180 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "loss: 1.541498  [    0/  700]\n",
            "loss: 1.558351  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.7%, Avg loss: 1.713590 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 2.011877 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "loss: 1.511368  [    0/  700]\n",
            "loss: 1.564472  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.4%, Avg loss: 1.710631 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.018505 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "loss: 1.542236  [    0/  700]\n",
            "loss: 1.546077  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.6%, Avg loss: 1.705589 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.009543 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "loss: 1.500377  [    0/  700]\n",
            "loss: 1.552762  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.6%, Avg loss: 1.702470 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 2.019737 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "loss: 1.537006  [    0/  700]\n",
            "loss: 1.544431  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.4%, Avg loss: 1.701646 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.999175 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "loss: 1.509342  [    0/  700]\n",
            "loss: 1.551743  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.4%, Avg loss: 1.700135 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 2.011159 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "loss: 1.524504  [    0/  700]\n",
            "loss: 1.536864  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.3%, Avg loss: 1.694666 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.000987 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "loss: 1.500294  [    0/  700]\n",
            "loss: 1.546258  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.0%, Avg loss: 1.693981 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 2.011857 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "loss: 1.526367  [    0/  700]\n",
            "loss: 1.535147  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.3%, Avg loss: 1.693075 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 2.000051 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "loss: 1.500926  [    0/  700]\n",
            "loss: 1.536899  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.9%, Avg loss: 1.687671 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 2.010093 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "loss: 1.520502  [    0/  700]\n",
            "loss: 1.530449  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.1%, Avg loss: 1.688355 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.995501 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "loss: 1.495539  [    0/  700]\n",
            "loss: 1.535080  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.4%, Avg loss: 1.683577 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.999095 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "loss: 1.526010  [    0/  700]\n",
            "loss: 1.532385  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.4%, Avg loss: 1.683900 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 2.003523 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "loss: 1.490448  [    0/  700]\n",
            "loss: 1.530917  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.7%, Avg loss: 1.683378 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 2.001933 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "loss: 1.516050  [    0/  700]\n",
            "loss: 1.521678  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.0%, Avg loss: 1.676957 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.995011 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "loss: 1.496943  [    0/  700]\n",
            "loss: 1.537148  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.6%, Avg loss: 1.682612 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.994257 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "loss: 1.509326  [    0/  700]\n",
            "loss: 1.505914  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.4%, Avg loss: 1.670653 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.997447 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "loss: 1.488966  [    0/  700]\n",
            "loss: 1.532074  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.4%, Avg loss: 1.673679 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.998581 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "loss: 1.504283  [    0/  700]\n",
            "loss: 1.497824  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.4%, Avg loss: 1.668892 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.976003 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "loss: 1.484376  [    0/  700]\n",
            "loss: 1.512524  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.1%, Avg loss: 1.662653 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.996200 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "loss: 1.497424  [    0/  700]\n",
            "loss: 1.514217  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.4%, Avg loss: 1.668334 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.993689 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "loss: 1.492301  [    0/  700]\n",
            "loss: 1.510026  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.1%, Avg loss: 1.665256 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 1.995196 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "loss: 1.498484  [    0/  700]\n",
            "loss: 1.505978  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.7%, Avg loss: 1.667034 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 1.982818 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "loss: 1.481931  [    0/  700]\n",
            "loss: 1.517748  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.7%, Avg loss: 1.660178 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 2.000950 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "loss: 1.502533  [    0/  700]\n",
            "loss: 1.495655  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.7%, Avg loss: 1.662712 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.991790 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "loss: 1.472203  [    0/  700]\n",
            "loss: 1.507885  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.4%, Avg loss: 1.656921 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.985166 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "loss: 1.509030  [    0/  700]\n",
            "loss: 1.496729  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.0%, Avg loss: 1.657288 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.982148 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "loss: 1.468564  [    0/  700]\n",
            "loss: 1.505347  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.7%, Avg loss: 1.652985 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.990764 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "loss: 1.492683  [    0/  700]\n",
            "loss: 1.490147  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.6%, Avg loss: 1.654274 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.976938 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "loss: 1.481410  [    0/  700]\n",
            "loss: 1.507706  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.6%, Avg loss: 1.650121 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.994746 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "loss: 1.481131  [    0/  700]\n",
            "loss: 1.496196  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.7%, Avg loss: 1.653534 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.985449 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "loss: 1.482093  [    0/  700]\n",
            "loss: 1.486190  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.6%, Avg loss: 1.642818 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.981409 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "loss: 1.478601  [    0/  700]\n",
            "loss: 1.499395  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.0%, Avg loss: 1.653157 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.982137 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "loss: 1.481863  [    0/  700]\n",
            "loss: 1.490899  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.4%, Avg loss: 1.638676 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.980809 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "loss: 1.468133  [    0/  700]\n",
            "loss: 1.485842  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.6%, Avg loss: 1.646042 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.984317 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "loss: 1.477423  [    0/  700]\n",
            "loss: 1.489473  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.4%, Avg loss: 1.639816 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.977183 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "loss: 1.476336  [    0/  700]\n",
            "loss: 1.465894  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.7%, Avg loss: 1.632700 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.949991 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "loss: 1.449809  [    0/  700]\n",
            "loss: 1.469064  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.7%, Avg loss: 1.622936 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.957679 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "loss: 1.445211  [    0/  700]\n",
            "loss: 1.481436  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.0%, Avg loss: 1.632391 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.991310 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "loss: 1.466994  [    0/  700]\n",
            "loss: 1.496321  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.9%, Avg loss: 1.637648 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.995472 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "loss: 1.472594  [    0/  700]\n",
            "loss: 1.468547  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.7%, Avg loss: 1.636585 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.969098 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "loss: 1.466435  [    0/  700]\n",
            "loss: 1.479297  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.9%, Avg loss: 1.631316 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.983458 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "loss: 1.465415  [    0/  700]\n",
            "loss: 1.474392  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.1%, Avg loss: 1.637988 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.975316 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "loss: 1.467005  [    0/  700]\n",
            "loss: 1.491010  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.0%, Avg loss: 1.626590 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.985832 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "loss: 1.465301  [    0/  700]\n",
            "loss: 1.452969  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.9%, Avg loss: 1.628194 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.976924 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "loss: 1.458644  [    0/  700]\n",
            "loss: 1.483030  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.9%, Avg loss: 1.630014 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.990485 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "loss: 1.469276  [    0/  700]\n",
            "loss: 1.472602  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.4%, Avg loss: 1.631674 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.971363 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "loss: 1.447850  [    0/  700]\n",
            "loss: 1.488447  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.1%, Avg loss: 1.624603 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.994126 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "loss: 1.471260  [    0/  700]\n",
            "loss: 1.456542  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.0%, Avg loss: 1.635260 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.0%, Avg loss: 1.972765 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "loss: 1.451254  [    0/  700]\n",
            "loss: 1.512421  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.6%, Avg loss: 1.631632 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.996469 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "loss: 1.462152  [    0/  700]\n",
            "loss: 1.487190  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.9%, Avg loss: 1.641833 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.965279 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "loss: 1.454945  [    0/  700]\n",
            "loss: 1.546932  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.0%, Avg loss: 1.639205 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 2.010981 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "loss: 1.477161  [    0/  700]\n",
            "loss: 1.483082  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.9%, Avg loss: 1.652312 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.969671 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "loss: 1.447538  [    0/  700]\n",
            "loss: 1.584870  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.6%, Avg loss: 1.641854 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 2.020786 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "loss: 1.480943  [    0/  700]\n",
            "loss: 1.500697  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.4%, Avg loss: 1.647022 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 40.3%, Avg loss: 1.960769 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "loss: 1.458232  [    0/  700]\n",
            "loss: 1.576836  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.1%, Avg loss: 1.636376 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 2.016779 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "loss: 1.472992  [    0/  700]\n",
            "loss: 1.498877  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.4%, Avg loss: 1.637668 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.977330 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "loss: 1.466301  [    0/  700]\n",
            "loss: 1.532945  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.9%, Avg loss: 1.629283 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 2.002581 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "loss: 1.462167  [    0/  700]\n",
            "loss: 1.467907  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.1%, Avg loss: 1.618833 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 40.3%, Avg loss: 1.973811 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "loss: 1.464424  [    0/  700]\n",
            "loss: 1.495439  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.0%, Avg loss: 1.620353 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 2.004674 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "loss: 1.454010  [    0/  700]\n",
            "loss: 1.455276  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.7%, Avg loss: 1.619755 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.968384 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "loss: 1.461440  [    0/  700]\n",
            "loss: 1.495067  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.0%, Avg loss: 1.619616 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 2.008692 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "loss: 1.457729  [    0/  700]\n",
            "loss: 1.449230  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.9%, Avg loss: 1.620154 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.942136 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "loss: 1.433750  [    0/  700]\n",
            "loss: 1.514023  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.0%, Avg loss: 1.618030 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.042813 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "loss: 1.473364  [    0/  700]\n",
            "loss: 1.456644  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.4%, Avg loss: 1.625661 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.991156 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "loss: 1.443117  [    0/  700]\n",
            "loss: 1.481861  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.4%, Avg loss: 1.638208 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 2.053546 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "loss: 1.512171  [    0/  700]\n",
            "loss: 1.468189  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.7%, Avg loss: 1.634091 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 1.971948 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "loss: 1.436706  [    0/  700]\n",
            "loss: 1.466329  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.0%, Avg loss: 1.619553 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.3%, Avg loss: 2.017422 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "loss: 1.486135  [    0/  700]\n",
            "loss: 1.466684  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.4%, Avg loss: 1.627658 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.999000 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "loss: 1.444277  [    0/  700]\n",
            "loss: 1.465289  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.3%, Avg loss: 1.631129 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 2.043860 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "loss: 1.472636  [    0/  700]\n",
            "loss: 1.478184  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.4%, Avg loss: 1.648123 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 2.027153 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "loss: 1.463510  [    0/  700]\n",
            "loss: 1.439665  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.1%, Avg loss: 1.652998 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 2.025503 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "loss: 1.457966  [    0/  700]\n",
            "loss: 1.515427  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.3%, Avg loss: 1.666980 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 2.040607 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "loss: 1.477926  [    0/  700]\n",
            "loss: 1.497397  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.4%, Avg loss: 1.694344 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 2.019060 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "loss: 1.469800  [    0/  700]\n",
            "loss: 1.617359  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.6%, Avg loss: 1.709667 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.071666 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "loss: 1.474902  [    0/  700]\n",
            "loss: 1.702937  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.1%, Avg loss: 1.763835 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.983299 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "loss: 1.551530  [    0/  700]\n",
            "loss: 1.729343  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.0%, Avg loss: 1.735261 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.998945 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "loss: 1.457818  [    0/  700]\n",
            "loss: 1.582016  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.0%, Avg loss: 1.693824 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.970017 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "loss: 1.505790  [    0/  700]\n",
            "loss: 1.662587  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 40.9%, Avg loss: 1.681339 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.0%, Avg loss: 1.979136 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "loss: 1.474595  [    0/  700]\n",
            "loss: 1.542808  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 39.6%, Avg loss: 1.662997 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.977089 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "loss: 1.510833  [    0/  700]\n",
            "loss: 1.548007  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 42.3%, Avg loss: 1.633030 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.944652 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "loss: 1.458981  [    0/  700]\n",
            "loss: 1.494007  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 41.7%, Avg loss: 1.619183 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 1.932613 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "loss: 1.481746  [    0/  700]\n",
            "loss: 1.477316  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.4%, Avg loss: 1.604322 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.944999 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "loss: 1.458147  [    0/  700]\n",
            "loss: 1.463377  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.1%, Avg loss: 1.600061 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.936378 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "loss: 1.461768  [    0/  700]\n",
            "loss: 1.442211  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 44.0%, Avg loss: 1.592097 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.941670 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "loss: 1.445731  [    0/  700]\n",
            "loss: 1.467229  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 44.7%, Avg loss: 1.587541 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.942032 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "loss: 1.448172  [    0/  700]\n",
            "loss: 1.436566  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 44.3%, Avg loss: 1.585632 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.936923 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "loss: 1.440261  [    0/  700]\n",
            "loss: 1.464971  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.4%, Avg loss: 1.589933 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 1.943986 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "loss: 1.455800  [    0/  700]\n",
            "loss: 1.446178  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.7%, Avg loss: 1.588426 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.0%, Avg loss: 1.936343 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "loss: 1.442150  [    0/  700]\n",
            "loss: 1.454316  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 44.1%, Avg loss: 1.583627 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.945348 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "loss: 1.446336  [    0/  700]\n",
            "loss: 1.441748  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 44.0%, Avg loss: 1.580500 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 38.7%, Avg loss: 1.936647 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "loss: 1.440039  [    0/  700]\n",
            "loss: 1.445968  [  300/  700]\n",
            "Training Error: \n",
            " Accuracy: 43.9%, Avg loss: 1.580857 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.942285 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-f13dd024c4f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#   print(x,y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#   break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ConfM_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c987c84b117a>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, epoch, mod)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprevLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# print(batch, X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# print(y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gTMxTXWcdMz"
      },
      "source": [
        "# %matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "animator.display_plt()\n",
        "# Automate improved filename description.\n",
        "animator.fig.savefig('loss.png', bbox_inches='tight')\n",
        "# plt.savefig('loss.png', bbox_inches='tight')   # Less specific. Targets active figure.\n",
        "\n",
        "# pipe labels are located in data to csv notebook\n",
        "# predictions = [f'{i}' for i in range(output_dim)]\n",
        "predictions = decode_labels()\n",
        "# labels = range(output_dim)\n",
        "labels = decode_labels()   # Update label list\n",
        "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
        "# fig.set_facecolor('#7d7f7c')\n",
        "im = ax.imshow(confusion_matrix)\n",
        "ax.set_xticks(np.arange(len(predictions)))\n",
        "ax.set_yticks(np.arange(len(labels)))\n",
        "ax.set_xticklabels(predictions)\n",
        "ax.set_yticklabels(labels)\n",
        "\n",
        "# Set-up for white grid lines on minor ticks. Creates spacing effect.\n",
        "ax.set_xticks(np.arange(len(predictions)+1) - 0.5, minor=True)\n",
        "ax.set_yticks(np.arange(len(labels)+1) - 0.5, minor=True)\n",
        "# Print white grid to space out the squares.\n",
        "ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=1)\n",
        "# Remove spines for clarity.\n",
        "for k, v in ax.spines.items() :\n",
        "  v.set_visible(False)\n",
        "# ax.spines['top'].set_visible(False)   # Can't slice a dictionary.\n",
        "ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "\n",
        "# Horizontal labeling displays on top\n",
        "ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
        "# Rotate tick labels and set alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=-45, ha='right', rotation_mode='anchor')\n",
        "plt.xlabel(f'Predictions -- {ts_size}')\n",
        "# Move the x labels to the top\n",
        "ax.xaxis.set_label_position('top')\n",
        "plt.ylabel('Labels')\n",
        "# Annotate matrix with values by looping over data dimensions\n",
        "for i in range(len(labels)) :\n",
        "  for j in range(len(predictions)) :\n",
        "    text = ax.text(j, i, confusion_matrix[i, j].item(),\n",
        "                   ha='center', va='center', color='white')\n",
        "\n",
        "ax.set_title(f'Confusion Matrix -- Epoch {t+1}')\n",
        "fig.tight_layout()\n",
        "# Save to file that is replaced on every run.\n",
        "fig.savefig('conf.png')\n",
        "# plt.show()\n",
        "# plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W6b5qaI8ZVj"
      },
      "source": [
        "####Sanity Check: Pass a sample to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp6zYEZN8mx5"
      },
      "source": [
        "def predict_ch3(net, sample, samp_idx=0):\n",
        "    \"\"\"Predict labels (redefined from d2l Chapter 3.6.7).\"\"\"\n",
        "    print('Model Evaluation')\n",
        "    X, y = sample[samp_idx]\n",
        "    X = X.reshape([1,-1])\n",
        "    preds = net(X).argmax(axis=1)\n",
        "    print(f'Scenario {samp_idx} (pred={preds.item()}, label={y.item()})')\n",
        "\n",
        "predict_ch3(model, tr_dataset, samp_idx=28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv4iOGftxLZi"
      },
      "source": [
        "# Test trained model on time stamps it hasn't seen before\n",
        "tmstp = 168\n",
        "X, y = cat_data(residual, norm_exp, norm_feats, mask, net_char, tmstp)\n",
        "ts_dataset = TensorDataset(X, y)\n",
        "\n",
        "test_dataloader = DataLoader(ts_dataset, batch_size=batch_size)\n",
        "test_acc = test_loop(test_dataloader, model, loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5xwqcQyNoBV"
      },
      "source": [
        "####Scratch Work"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( torch.log(torch.exp(torch.tensor(1))) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsHyPXChGzAj",
        "outputId": "8fe04d0c-de3a-4bf7-d3c0-473029bd8930"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f1(l_list) :\n",
        "  for l in l_list :\n",
        "    print(l)\n",
        "\n",
        "f1((1,2))"
      ],
      "metadata": {
        "id": "HEA0CSvN9UxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9aks4N-SHt5"
      },
      "source": [
        "t = torch.rand(10, generator=torch.Generator().manual_seed(10))\n",
        "print(' t:', t)\n",
        "idxs = torch.randperm(5, generator=torch.Generator().manual_seed(10))\n",
        "print(idxs)\n",
        "tn = t[idxs]\n",
        "print(' t:', t)\n",
        "print('tn:', tn)\n",
        "print(' t:', t[2])\n",
        "print('tn:', tn[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpjdHiXWtwOl"
      },
      "source": [
        "y_hat = torch.arange(20).reshape([2, -1])\n",
        "print(y_hat)\n",
        "len(y_hat)\n",
        "print(y_hat.sum(1))\n",
        "print(y_hat.argmax(dim=0))\n",
        "print(y_hat.argmax(dim=1))\n",
        "data = [[1.0, 1.0], [1.0, 1.0]] * 2   # multiplies the number of elements (like if you had 2 apples and then multiplied them by 2; you now have four apples)\n",
        "print(data)\n",
        "# print(data / data)   # dividing a list is not defined\n",
        "(1,2) + (3,)   # cats the three elems\n",
        "len((1,2))   # tuples have __len__ defined\n",
        "[[] for _ in range(3)]\n",
        "rows = [[1,1]]\n",
        "print(rows)\n",
        "[rows.append(i) for i in [[3,3],[4,4]]]\n",
        "print(rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPQBiMXCOahN"
      },
      "source": [
        "# X_masked = None\n",
        "# masked_feats = torch.rand(15)\n",
        "# print(masked_feats)\n",
        "# mask = torch.randint(2, [15])\n",
        "# print(mask)\n",
        "# # Mask and masked features\n",
        "# # May want sensing_mask_rand() to process batches of samples\n",
        "# for i in range(5):\n",
        "#   temp = torch.cat((masked_feats, mask)).reshape([1,-1])\n",
        "#   print(temp)\n",
        "\n",
        "#   if X_masked is None:\n",
        "#     X_masked = temp   \n",
        "#     print(X_masked)\n",
        "#   else:  \n",
        "#     X_masked = torch.cat((X_masked, temp))\n",
        "#     print(X_masked)\n",
        "\n",
        "# for i in X_masked:\n",
        "#   print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4kiAKeKfBIp"
      },
      "source": [
        "# size = [100,]\n",
        "# K = 20\n",
        "# tn = torch.zeros(size)\n",
        "# mask = torch.zeros(tn.size())\n",
        "# print(mask.size())\n",
        "# print(mask)\n",
        "# indices = torch.randint(len(tn), size=(K,))\n",
        "# print(indices)\n",
        "# for idx in indices:\n",
        "#   mask[idx] = 1\n",
        "# print(mask)\n",
        "\n",
        "# mask = torch.cuda.FloatTensor(3, 3).uniform_()\n",
        "# # tensor of floats\n",
        "# mask = torch.FloatTensor(3,3).uniform_()\n",
        "# print(mask)\n",
        "# # tensor of booleans (?? how ??)\n",
        "# mask = torch.FloatTensor(3,3).uniform_() > 0.8\n",
        "# print(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxvQSxfCAX_R"
      },
      "source": [
        "# converting string labs to labels ranging from 0 -> num_of_classes (i.e. possible leak locations)\n",
        "#  what if not all of the possible leak locations are used?\n",
        "#  1) I can set the output dim to len of label_subset (easiser)\n",
        "#  2) I can force the set to be all the possible fixed pipe locations (coordinating this will be tricky)\n",
        "# labs = [1,2,2,3,1,4,4,3]\n",
        "# lab_dict = {}\n",
        "# encoded_labs = []\n",
        "# label_subset = set(labs)\n",
        "# print(label_subset)\n",
        "# print(type(label_subset))\n",
        "# print(len(label_subset))\n",
        "# for i, key in enumerate(label_subset):\n",
        "#   lab_dict[key] = i\n",
        "# print(lab_dict)\n",
        "# for key in labs:\n",
        "#   encoded_labs.append(lab_dict[key])\n",
        "# print(encoded_labs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snpJEOWKoc5Y"
      },
      "source": [
        "# Reshaping practice\n",
        "# base_file = 'simdata/_base_/node_demand.csv'\n",
        "# data_file = base_file\n",
        "# data = pd.read_csv(data_file)\n",
        "# data_tn = torch.tensor(data.values, dtype=torch.float32)\n",
        "# data_tn[:,1:].reshape([1,-1])\n",
        "\n",
        "# data_tn = torch.arange(20).reshape([4,5])\n",
        "# print(data_tn)\n",
        "# data_tn = data_tn[:,1:].reshape([1,-1])\n",
        "# print(data_tn)\n",
        "# data_tn1 = torch.arange(20).reshape([4,5])\n",
        "# print(data_tn1)\n",
        "# data_tn1 = data_tn1[:,1:].reshape([1, -1])\n",
        "# print(data_tn1)\n",
        "# torch.cat((data_tn1, data_tn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DGW06k6V-qI"
      },
      "source": [
        "# Extracting an intelligible answer from the model\n",
        "# x = torch.arange(16, dtype=torch.float32).reshape((4,4))\n",
        "# print(x)\n",
        "# print(x.sum(axis=0))\n",
        "# print(x.sum(axis=[0,1]))\n",
        "# mean = x.sum() / x.numel()\n",
        "# print(mean)\n",
        "# # notice we keep all dims (tensor of a tensor ie. two brackets) vs above we lost one (just a tensor)\n",
        "# print(x.sum(dim=0, keepdim=True))\n",
        "\n",
        "# y = torch.tensor([3,3,3,3])\n",
        "# # x.argmax(1).type(y.dtype) == y\n",
        "# correct = 0\n",
        "# correct += (x.argmax(1).type(y.dtype) == y).type(torch.float32).sum().item()\n",
        "# correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kndCfp0DOku8"
      },
      "source": [
        "# Handy timer class\n",
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3m7r6ps_rDM"
      },
      "source": [
        "# Target transform\n",
        "from torchvision.transforms import Lambda\n",
        "\n",
        "train_size = 700\n",
        "# target_transform = Lambda(lambda y: torch.zeros(\n",
        "#     (train_size, output_dim), dtype=torch.float).scatter_(\n",
        "#         dim=1, index=torch.randint(low=0,high=output_dim,size=[train_size,1]), value=1))\n",
        "\n",
        "# one-hot classification label vector\n",
        "target_transform = Lambda(lambda y: torch.scatter_(\n",
        "        dim=1, index=torch.randint(low=0,high=output_dim,size=[train_size,1]), value=1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}